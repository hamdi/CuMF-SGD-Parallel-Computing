{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5858d576-c4d7-4b22-ab1d-0d6290d31d64",
   "metadata": {},
   "source": [
    "## Factorisation de matrices par descente de gradient: Parallélisation et optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f4b3e-bfa7-45b4-a5f1-89a2670aad9e",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "L'objectif de ce projet est de paralléliser la factorisation de matrices en se basant sur l'approche SGD proposée dans le papier CuMF_SGD: Parallelized Stochastic Gradient Descent for Matrix Factorization on GPUs [1].\n",
    "\n",
    "On utilisera le dataset Netflix, un benchmark standard de la factorisation de matrices. Ce dataset est fourni sous forme d'une matrice sparse (i,j,R[i,j]) où i est l'identifiant d'un utilisateur, j est l'identifiant d'un film et R[i,j] est l'avis (R comme rating) de l'utilisateur i sur le film j.\n",
    "\n",
    "On commence par charger les données. On peut les garder sous leur format sparse original ou on peut les transformer en une matrice R complète (ayant pour lignes les identifiants des utilisateur et en colonnes ceux des films)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b61d318-68b9-4dc0-ad47-84845aa8fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "from pycuda.compiler import SourceModule\n",
    "from pycuda import gpuarray\n",
    "from IPython.display import clear_output\n",
    "import cupy as cp\n",
    "from time import time\n",
    "from numba import jit\n",
    "np.random.seed(0)\n",
    "cp.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd32bf1a-9d37-4649-a4d6-6248d32317e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful to capture C stdout and print it in Jupyter\n",
    "%load_ext wurlitzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04272783-ad12-4b46-a4a2-0ba7e2260f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(mode = \"sparse\", n=-1, file = \"netflix_test.txt\", shuffle=True):\n",
    "    \"\"\"\n",
    "    Load the data\n",
    "    mode: 'sparse' for sample format (i, j, R[i,j]), 'full' for full R matrix\n",
    "    n: number of rows to load from file; set negative to load all the data\n",
    "    shuffle: Randomly shuffle the samples\n",
    "    \"\"\"\n",
    "    data_sep = \" \" if file==\"netflix_test.txt\" else \",\"\n",
    "    # Load data from file\n",
    "    data = np.genfromtxt(\"netflix_test.txt\", delimiter=data_sep, dtype=\"int32\")\n",
    "    data = data[:,[0,1,3]]\n",
    "    # set number of rows to load\n",
    "    if n<=0 or n>len(data):\n",
    "        n = len(data)\n",
    "    # shuffle\n",
    "    if shuffle:\n",
    "        np.random.shuffle(data)\n",
    "    # Convert to the defined mode\n",
    "    if mode==\"sparse\":\n",
    "        R = data[:n,:]\n",
    "    elif mode==\"full\":\n",
    "        data = data[:n,:]\n",
    "        rows, row_pos = np.unique(data[:, 0], return_inverse=True)\n",
    "        cols, col_pos = np.unique(data[:, 1], return_inverse=True)\n",
    "        R = np.zeros((len(rows), len(cols)), dtype=\"uint8\")\n",
    "        R[row_pos, col_pos] = data[:, 2]\n",
    "    else:\n",
    "        print(\"mode should be 'sparse' or 'full'\")\n",
    "        return(-1)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea2a7f-22ce-49c9-b51b-8cad2eaa2223",
   "metadata": {},
   "source": [
    "Aperçu des données en mode sparse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbaa1291-e2aa-4993-b28e-a86af6622e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[313306,  11077,      4],\n",
       "       [336800,   4588,      2],\n",
       "       [177586,   8782,      5],\n",
       "       ...,\n",
       "       [ 69205,   9913,      3],\n",
       "       [144193,    660,      4],\n",
       "       [364936,   1975,      4]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = load_data(\"sparse\"); R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b01c0509-3285-48e7-88d9-f3c5ee5fe0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R a les dimensions (1408395, 3) et occupe 16 MB en mémoire\n"
     ]
    }
   ],
   "source": [
    "print(f\"R a les dimensions {R.shape} et occupe {R.nbytes//1024//1024} MB en mémoire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dc859d-e0f0-4ce0-851e-1d4b31a1be37",
   "metadata": {},
   "source": [
    "Aperçu des données en mode complet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44ba6ad7-a153-427e-adba-39ac94d0c073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = load_data(\"full\"); R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "457b5cc4-6104-4f8a-9ced-6a832673ba20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R a les dimensions (462858, 16938) et occupe 7476 MB en mémoire\n"
     ]
    }
   ],
   "source": [
    "print(f\"R a les dimensions {R.shape} et occupe {R.nbytes//1024//1024} MB en mémoire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276affcf-0eec-4716-a762-5efce4f41678",
   "metadata": {},
   "source": [
    "Le mode sparse occupe beaucoup moins de mémoire, donc on le privilégie pour la suite. Mais d'abord nous fournissons un petit exemple avec une matrice complète pour mieux visualiser la factorisation de matrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8ba2008-f031-4bfb-84f4-673531c269b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 0, 5],\n",
       "       [2, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 4, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [3, 0, 3, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = load_data(\"full\", 5)\n",
    "R[0,0] = 2; R[4,0]=3  # R is too sparse, we add a few items to make it more meaningful\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f452a6-5778-471e-84f5-b19c94405577",
   "metadata": {},
   "source": [
    "On implémente une version basique de la factorisation de matrices avec descente de gradient. L'objectif est de calculer deux matrices P et Q dont le produit est approximativement égal à R sur ses éléments non nuls. Cela permet aussi de donner une estimation des éléments nuls (manquants) de R. Pour cette raison cette technique est fréquemment utilisée dans les systèmes de recommandation.\n",
    "\n",
    "Dans toute la suite, les hyperparamètres alpha (learning rate) et beta (régularisation) ont été optimisés manuellement pour donner des résultats optimaux pour chaque algorithme. Le paramètre k (dimension des facteurs représentant les utilisateurs et les films) est fixé à 128 pour une comparaison plus pertinente avec la méthode CuMF de l'article [1] qui nécessite k=128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1fce234-6a00-4709-bd90-2760459590bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_full_matrix(R, R_pred):\n",
    "    \"\"\"\n",
    "    Calculate the root mean squared error between two matrices on non-zero elements\n",
    "    \"\"\"\n",
    "    i_s, j_s = R.nonzero()\n",
    "    sse = np.sum((R[i_s, j_s] - R_pred[i_s, j_s])**2)\n",
    "    return np.sqrt(sse/len(i_s))\n",
    "\n",
    "def sgd_iteration(P, Q, samples, alpha, beta):\n",
    "    \"\"\"\n",
    "    SGD iteration over the provided samples from R\n",
    "    \"\"\"\n",
    "    for i, j, r in samples:\n",
    "        # Compute prediction and error\n",
    "        prediction = P[i, :].dot(Q[j, :].T)\n",
    "        e = (r - prediction)\n",
    "\n",
    "        # Update user and item feature matrices\n",
    "        P[i, :] += alpha * (e * Q[j, :] - beta * P[i,:])\n",
    "        Q[j, :] += alpha * (e * P[i, :] - beta * Q[j,:])\n",
    "    return(P,Q)\n",
    "\n",
    "def mf_sgd_full_matrix(R, k, alpha, beta, n_iter, verbose = True):\n",
    "    \"\"\"\n",
    "    Matrix factorization\n",
    "    - R : matrix to factorize\n",
    "    - k : latent dimension\n",
    "    - alpha : learning rate\n",
    "    - beta : regularization parameter\n",
    "    - n_iter : number of SGD iterations\n",
    "    \"\"\"\n",
    "    # Initialize user and item feature matrices\n",
    "    m,n = R.shape\n",
    "    P = 1./k * np.random.normal(size=(m,k))\n",
    "    Q = 1./k * np.random.normal(size=(n,k))\n",
    "\n",
    "    # Create a list of training samples\n",
    "    samples = [(i, j, R[i,j]) for i in range(m) for j in range(n) if R[i, j] > 0]\n",
    "\n",
    "    # Perform stochastic gradient descent for number of iterations\n",
    "    for i in range(n_iter):\n",
    "        P,Q = sgd_iteration(P, Q, samples, alpha, beta)\n",
    "        if verbose and ((i+1)%(n_iter//10) == 0):\n",
    "            print(f\"Iteration: {i+1}: RMSE = {rmse_full_matrix(R, P.dot(Q.T)):.5f}\")\n",
    "    return(P,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9711714a-7c74-4a50-a98c-ae7924829a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 100: RMSE = 0.29382\n",
      "Iteration: 200: RMSE = 0.09416\n",
      "Iteration: 300: RMSE = 0.03717\n",
      "Iteration: 400: RMSE = 0.01514\n",
      "Iteration: 500: RMSE = 0.00621\n",
      "Iteration: 600: RMSE = 0.00258\n",
      "Iteration: 700: RMSE = 0.00111\n",
      "Iteration: 800: RMSE = 0.00052\n",
      "Iteration: 900: RMSE = 0.00029\n",
      "Iteration: 1000: RMSE = 0.00020\n",
      "Temps d'exécution: 0.072 s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "P,Q = mf_sgd_full_matrix(R, k=2, alpha=0.01, beta=0.0001, n_iter=1000)\n",
    "end = time()\n",
    "print(f\"Temps d'exécution: {(end-start):.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82a95644-ae30-45b9-a7de-ba1ce3a0b976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.69867332,  0.16426039],\n",
       "       [ 1.59920733,  0.33895919],\n",
       "       [-1.63958472,  1.18567952],\n",
       "       [-1.21138918,  0.28725432],\n",
       "       [ 2.00630722,  1.20073611]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa1de2f3-3e54-459d-9b2a-39fe0d8f413d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.00032158, -1.10017641,  0.83007038, -2.49560702,  4.99978396],\n",
       "       [ 1.99984396, -0.9376086 ,  1.15121634, -2.13130822,  4.82492808],\n",
       "       [-1.08000033,  1.77769254,  1.89539962,  3.99990004, -3.96596913],\n",
       "       [-1.17057102,  0.99991389,  0.21928431,  2.25837799, -3.30684811],\n",
       "       [ 2.99972706, -0.76334935,  2.99995274, -1.75594684,  6.54925722]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.dot(Q.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a81a3c-9111-482d-aad6-cda5c4c82ffa",
   "metadata": {},
   "source": [
    "Sur une petite matrice on constate que cet algorithme de factorisation SGD permet d'obtenir rapidement une factorisation avec un taux d'erreur inférieur à 0.1%. Réessayons avec une taille plus élevée:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bfde08f6-36e4-46bb-83fe-3736e1a14036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91130, 9813)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = load_data(\"full\", 100000)\n",
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c34942f-747b-4f24-b556-03d05a1840ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5: RMSE = 2.75844\n",
      "Iteration: 10: RMSE = 1.88797\n",
      "Iteration: 15: RMSE = 1.37002\n",
      "Iteration: 20: RMSE = 1.03478\n",
      "Iteration: 25: RMSE = 0.79666\n",
      "Iteration: 30: RMSE = 0.61906\n",
      "Iteration: 35: RMSE = 0.49134\n",
      "Iteration: 40: RMSE = 0.40592\n",
      "Iteration: 45: RMSE = 0.35006\n",
      "Iteration: 50: RMSE = 0.31246\n",
      "Temps d'exécution: 831.640 s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "P,Q = mf_sgd_full_matrix(R, k=128, alpha=0.025, beta=0.2, n_iter=50)\n",
    "end = time()\n",
    "print(f\"Temps d'exécution: {(end-start):.3f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3378261d-aba0-4b6d-9b72-1df9223d04b7",
   "metadata": {},
   "source": [
    "Le résultat n'est pas le même. Calculons le nombre d'updates/s : On appelle un update (ou pas de gradient) la lecture d'un échantillon (i,j,R[i,j]) de R et la mise à jour des matrices P et Q à partir de cet unique échantillon. D'abord réexécutons le code sans calcul intermédiaire de RMSE pour obtenir une valeur plus fiable du temps d'exécution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "367b7df2-8016-40d3-b838-a9d9cfae3ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13min 19s ± 4.72 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "P,Q = mf_sgd_full_matrix(R, k=128, alpha=0.025, beta=0.2, n_iter=50, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f60fb2f-3855-4e85-b087-2843ceb8afff",
   "metadata": {},
   "source": [
    "Ici on a $n_{iterations} \\times n_{samples} = 50 \\times 100\\,000 = 5\\,000\\,000$ updates de gradient en 799s, ce qui revient à 6258 updates/s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d51420-a167-4202-be83-7e4b18a10c31",
   "metadata": {},
   "source": [
    "### Format sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e543fb15-bda2-4ae4-ac6b-37c4b2945c3f",
   "metadata": {},
   "source": [
    "Une première optimisation consiste à utiliser une matrice R sous format sparse, ce qui réduit son empreinte mémoire et le temps de récupération des échantillons pour la descente de gradient. Pour optimiser l'empreinte mémoire de P et Q, on applique également une bijection sur les identifiants des utilisateurs afin d'obtenir des identifiants allant de 0 à $n_{utilisateurs}$ et de même pour les identifiants des films. Cela évite les gaps dans les identifiants et donc d'avoir des features inutilisés dans P et Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b52e219e-9bb4-4d8a-a482-75da36a31b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_ids(R):\n",
    "    R[:,0] = np.unique(R[:,0],return_inverse = True)[1]\n",
    "    R[:,1] = np.unique(R[:,1],return_inverse = True)[1]\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "277c9070-84f1-4164-916d-03830324b9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[61493,  6340,     4],\n",
       "       [66136,  2810,     2],\n",
       "       [34656,  5160,     5],\n",
       "       ...,\n",
       "       [57710,   503,     4],\n",
       "       [77190,   538,     4],\n",
       "       [ 7831,   132,     5]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = load_data(\"sparse\", 100000)\n",
    "R = remap_ids(R); R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81ae2a9b-4dda-4e52-9ca3-33a60dc63ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(R, P, Q):\n",
    "    \"\"\"\n",
    "    Calculate the root mean square error between two matrices on non-zero elements\n",
    "    \"\"\"\n",
    "    sse = 0 # sum of squared errors\n",
    "    for i, j, r in R:\n",
    "        prediction = P[i, :].dot(Q[j, :].T)\n",
    "        sse += (r - prediction)**2\n",
    "    return np.sqrt(sse/len(R))\n",
    "\n",
    "def mf_sgd(R, k, alpha, beta, n_iter, verbose = True):\n",
    "    \"\"\"\n",
    "    Matrix factorization\n",
    "    - R : matrix to factorize\n",
    "    - k : latent dimension\n",
    "    - alpha : learning rate\n",
    "    - beta : regularization parameter\n",
    "    - n_iter : number of SGD iterations\n",
    "    \"\"\"\n",
    "    # Initialize user and item feature matrices\n",
    "    m,n = int(1+np.max(R[:,0])), int(1+np.max(R[:,1]))\n",
    "    P = 1./k * np.random.normal(size=(m,k))\n",
    "    Q = 1./k * np.random.normal(size=(n,k))\n",
    "\n",
    "    # Perform stochastic gradient descent for number of iterations\n",
    "    for i in range(n_iter):\n",
    "        P,Q = sgd_iteration(P, Q, R, alpha, beta)\n",
    "        if verbose and ((i+1)%(n_iter//10) == 0):\n",
    "            print(f\"Iteration: {i+1}: RMSE = {rmse(R, P, Q):.5f}\")\n",
    "    return(P,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19f53867-c576-48cb-9309-b149d7eb0981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5: RMSE = 2.77869\n",
      "Iteration: 10: RMSE = 1.89984\n",
      "Iteration: 15: RMSE = 1.37656\n",
      "Iteration: 20: RMSE = 1.03226\n",
      "Iteration: 25: RMSE = 0.79341\n",
      "Iteration: 30: RMSE = 0.61939\n",
      "Iteration: 35: RMSE = 0.49561\n",
      "Iteration: 40: RMSE = 0.41257\n",
      "Iteration: 45: RMSE = 0.35712\n",
      "Iteration: 50: RMSE = 0.31903\n",
      "Temps d'exécution: 55.213 s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "P,Q = mf_sgd(R, k=128, alpha=0.025, beta=0.2, n_iter=50)\n",
    "end = time()\n",
    "print(f\"Temps d'exécution: {(end-start):.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf17c675-43bc-4b35-ac1f-d6c1c58ae224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.2 s ± 444 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "P,Q = mf_sgd(R, k=128, alpha=0.025, beta=0.2, n_iter=50, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b73ed-d3d8-4137-8c16-8b6ad8f7a429",
   "metadata": {},
   "source": [
    "Ici on a 5 000 000 updates comme avant mais en 52.2 s, soit 95785 updates/s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ec0634-cec5-45e4-8c9f-d30bc12657d8",
   "metadata": {},
   "source": [
    "### Numba\n",
    "\n",
    "Dans cette partie on utilise Numba en mode Just-in-time pour accélérer le code. Numba utilise le compilateur LLVM pour compiler nos fonctions en code machine qui est appelé à chaque exécution. Numba est compatible avec la majorité des objets et des fonctions de Numpy et permet de paralléliser leur exécution si cela est possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57b7ebd4-3a81-4673-8b91-1dcd2e2fb6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def rmse(R, P, Q):\n",
    "    \"\"\"\n",
    "    Calculate root mean square error between two matrices on non-zero elements\n",
    "    \"\"\"\n",
    "    sse = 0 # sum of squared errors\n",
    "    for i, j, r in R:\n",
    "        prediction = P[i, :].dot(Q[j, :].T)\n",
    "        sse += (r - prediction)**2\n",
    "    return np.sqrt(sse/len(R))\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def sgd_iteration(P, Q, R, alpha, beta):\n",
    "    \"\"\"\n",
    "    SGD step for matrix factorization\n",
    "    \"\"\"\n",
    "    for i, j, r in R:\n",
    "        # Compute prediction and error\n",
    "        prediction = P[i, :].dot(Q[j, :].T)\n",
    "        e = (r - prediction)\n",
    "\n",
    "        # Update user and item latent feature matrices\n",
    "        P[i, :] += alpha * (e * Q[j, :] - beta * P[i,:])\n",
    "        Q[j, :] += alpha * (e * P[i, :] - beta * Q[j,:])\n",
    "    return(P,Q)\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def mf_sgd(R, k, alpha, beta, n_iter, verbose = True):\n",
    "    \"\"\"\n",
    "    Matrix factorization\n",
    "    - R : matrix to factorize\n",
    "    - k : latent dimension\n",
    "    - alpha : learning rate\n",
    "    - beta : regularization parameter\n",
    "    - n_iter : number of SGD iterations\n",
    "    \"\"\"\n",
    "    # Initialize user and item latent feature matrice\n",
    "    m,n = (1+np.max(R[:,0])), (1+np.max(R[:,1]))\n",
    "    P = 1./k * np.array([np.random.normal() for i in range(m*k)]).reshape(m,k)\n",
    "    Q = 1./k * np.array([np.random.normal() for i in range(n*k)]).reshape(n,k)\n",
    "\n",
    "    # Perform stochastic gradient descent for number of iterations\n",
    "    for i in range(n_iter):\n",
    "        P,Q = sgd_iteration(P, Q, R, alpha, beta)\n",
    "        if verbose and ((i+1)%(n_iter//10) == 0):\n",
    "            print(\"Iteration: \",i+1,\": RMSE = \",rmse(R, P, Q))\n",
    "    return(P,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6037658b-c263-435d-a197-5e603fccdf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  5 : RMSE =  2.7807619324090216\n",
      "Iteration:  10 : RMSE =  1.9105106239856737\n",
      "Iteration:  15 : RMSE =  1.3850826465886725\n",
      "Iteration:  20 : RMSE =  1.042221850082097\n",
      "Iteration:  25 : RMSE =  0.8058066044542925\n",
      "Iteration:  30 : RMSE =  0.634046730075755\n",
      "Iteration:  35 : RMSE =  0.5128457058783094\n",
      "Iteration:  40 : RMSE =  0.43180818421416756\n",
      "Iteration:  45 : RMSE =  0.3778885842010513\n",
      "Iteration:  50 : RMSE =  0.34063762536269115\n",
      "Temps d'exécution: 30.410 s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "P,Q = mf_sgd(R, k=128, alpha=0.025, beta=0.2, n_iter=50)\n",
    "end = time()\n",
    "print(f\"Temps d'exécution: {(end-start):.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c89543bc-241e-4906-add1-c8e40f472adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.3 s ± 2.24 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "P,Q = mf_sgd(R, k=128, alpha=0.025, beta=0.2, n_iter=50, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c2387b4-c118-49aa-b306-912ac2d88e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  50 : RMSE =  1.8298425738661261\n",
      "Iteration:  100 : RMSE =  0.9457960716981967\n",
      "Iteration:  150 : RMSE =  0.5016145296472588\n",
      "Iteration:  200 : RMSE =  0.2676194944897138\n",
      "Iteration:  250 : RMSE =  0.15124073111221237\n",
      "Iteration:  300 : RMSE =  0.09561694689326262\n",
      "Iteration:  350 : RMSE =  0.06477346322931872\n",
      "Iteration:  400 : RMSE =  0.04814136577128693\n",
      "Iteration:  450 : RMSE =  0.03965336796342305\n",
      "Iteration:  500 : RMSE =  0.033144101548701385\n",
      "Temps d'exécution: 344.880 s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "P,Q = mf_sgd(R, k=128, alpha=0.005, beta=0.001, n_iter=500)\n",
    "end = time()\n",
    "print(f\"Temps d'exécution: {(end-start):.3f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37b9e28-ceb9-455a-a013-a3e7b3316019",
   "metadata": {},
   "source": [
    "Avec Numba on obtient un temps d'exécution moyen de 32.3s pour 5 000 000 updates, soit 154 800 updates/s.\n",
    "\n",
    "L'utilisation de Numba a fourni ici un speed-up de 62% par rapport au code Python précédent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0ff35b-7d73-4908-a08b-56e2a83132e4",
   "metadata": {},
   "source": [
    "### Parallélisation et approche Hogwild!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c34e669-d488-4ff0-b500-c3f0b0e1742a",
   "metadata": {},
   "source": [
    "Dans la partie suivante on se propose de paralléliser la descente de gradient sur GPU. On utilise l'approche Hogwild![2] qui est l'une des approches les plus populaires pour paralléliser la descente de gradient, non pas seulement de par sa simplicité mais aussi par sa rapidité. Cette approche suppose que le problème d'optimisation est sparse, c'est-à-dire que la plupart des updates de gradient ne modifient qu'un sous-ensemble de paramètres de taille largement inférieure au nombre de threads qui s'exécutent en parallèle. Dans ce cas, la probabilité de collision est faible, et même en cas de collision la perte de précision résultante est négligeable.\n",
    "\n",
    "On précise qu'ici, on parle de collision (ou race condition) lorsqu'un thread (1) et un thread (2) traitent en parallèle deux observations ayant la même ligne i ou la colonne j dans la matrice R, et dans ce cas ils modifient le même facteur P[i] ou Q[j] (supposons que c'est P[i]), et si le thread (1) effectue sa modification de P[i] entre la lecture du thread (2) de P[i] et la modification de (2) de P[i], alors la modification apportée par le thread (1) sera écrasée par celle du thread (2) et on perd un update.\n",
    "\n",
    "La notion de collision étant précisée, on voit que pour les problèmes sparses la probabilité de collision est faible et son impact négligeable, d'où l'idée de Hogwild! consistant simplement à paralléliser les updates de gradient entre les threads en traitant une observation aléatoire par thread, sans tenir compte des potentielles collisions. Cela évite le besoin d'un scheduler pour choisir l'observation à traiter par chaque thread de manière à éviter les collisions (dans ce cas le scheduler sera le bottleneck et ralentira l'exécution selon l'article [1]), et évite le besoin de synchroniser les threads ou de verrouiller des accès mémoire. Cette approche est donc optimale de point de vue performance mais au coût d'une perte de précision inversement proportionnelle à la sparsité du problème (perte de précision nulle dans le cas théorique d'une matrice infiniment sparse, mais élevée si le nombre de threads concurrents est supérieur au nombre d'observations indépendantes, c'est-à-dire ne partageant pas une même ligne ou colonne dans R).\n",
    "\n",
    "Dans notre cas la matrice R est très sparse avec seulement 0.01% de ses éléments étant non nuls (c'est généralement le cas des problèmes de recommandation), ce qui en fait un problème parfaitement adapté à l'approche Hogwild!. On adopte donc cette méthode, qu'on optimisera ensuite comme proposé dans [1] en traitant les données par batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "806bfd74-40ad-4e07-92f8-9ef775626e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00011295908037319723"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sparsity of R: number of non-zero entries / total number of entries\n",
    "len(R)/((1+np.max(R[:,0]))*(1+np.max(R[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ced08-fd7e-41d0-a3b0-7df99c357f86",
   "metadata": {},
   "source": [
    "### PyCuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95433bd3-6d9e-48b6-960a-5d5ad027affb",
   "metadata": {},
   "source": [
    "Dans cette partie on utilise PyCuda pour paralléliser la factorisation de matrices avec descente de gradient sur un nombre paramétrable de threads et de blocs.\n",
    "\n",
    "On applique d'abord l'approche Hogwild! standard où à chaque itération, chaque thread lit une observation aléatoire de R et effectue une mise à jour de gradient sur P et Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "47c8efee-3db6-436a-9fd4-b8a413de4187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26918,  3614,     4],\n",
       "       [30028,  3686,     2],\n",
       "       [47327,  9157,     4],\n",
       "       ...,\n",
       "       [77065,  3781,     4],\n",
       "       [38373,  1160,     4],\n",
       "       [75980,  8320,     4]], dtype=int32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = load_data(\"sparse\", 100000)\n",
    "R = remap_ids(R); R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "1bfa3260-8ddd-4a12-acc4-857fb68d4e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pycudakernel = SourceModule(\"\"\"\n",
    "#include <stdio.h>\n",
    "__global__ void pycuda_mf_kernel(float *P, float *Q, float alpha, float beta, int k, int n_samples, float *samples, int n_iter,\n",
    "int n_threads_per_block,int n_blocks, int *rndm)\n",
    "{\n",
    "    // variable initialization\n",
    "    int sample, iter;\n",
    "    int thread_id_within_block = threadIdx.x;\n",
    "    int block_id = blockIdx.x;\n",
    "    int global_thread_id = block_id*n_threads_per_block + thread_id_within_block; // bijection that associates a unique id to every (thread,block) couple\n",
    "    // we iterate n_iter times\n",
    "    for ( iter = 0; iter < n_iter; iter++) {\n",
    "        // every iteration, we pick a random sample from R and perform an gradient update\n",
    "        sample = rndm[global_thread_id*n_iter+iter]; // get a new random number every iteration for every thread\n",
    "        // read sample (i,j,R[i,j])\n",
    "        int i = samples[sample*3];\n",
    "        int j = samples[sample*3+1];\n",
    "        float r = samples[sample*3+2];\n",
    "        // calculate prediction and error\n",
    "        float pred = 0;\n",
    "        int t;\n",
    "        for( t = 0; t < k; t = t + 1 ){\n",
    "        pred =pred+(P[i*k+t]*Q[j*k+t]);\n",
    "        }\n",
    "        float e = r - pred;\n",
    "        // update P[i] and Q[j]\n",
    "        for( t = 0; t < k; t = t + 1 ){\n",
    "            P[i*k+t] = alpha * e * Q[j*k+t] + (1 - alpha * beta) * P[i*k+t];\n",
    "            Q[j*k+t] = alpha * e * P[i*k+t] + (1 - alpha * beta) * Q[j*k+t];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "pycuda_mf_kernel = pycudakernel.get_function(\"pycuda_mf_kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "a1ee8543-6889-491b-a35f-142e291ba85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mf_sgd_pycuda(R, k, alpha, beta, n_iter, n_threads_per_block = 32, n_blocks = 1):\n",
    "    \"\"\"\n",
    "    Matrix factorization\n",
    "    - R : matrix to factorize\n",
    "    - k : latent dimension\n",
    "    - alpha : learning rate\n",
    "    - beta : regularization parameter\n",
    "    - n_iter : number of SGD iterations\n",
    "    - n_threads_per_block : number of threads per block, each thread will run the kernel code\n",
    "    - n_blocks : number of blocks (groups of threads)\n",
    "    \"\"\"\n",
    "    # Copy the needed arrays to GPU memory\n",
    "    m = 1 + np.max(R[:,0])\n",
    "    n = 1 + np.max(R[:,1])\n",
    "    P = gpuarray.to_gpu((1./k * np.random.normal(size = (m,k))).astype(\"float32\"))\n",
    "    Q = gpuarray.to_gpu((1./k * np.random.normal(size = (n,k))).astype(\"float32\"))\n",
    "    R_gpu = gpuarray.to_gpu(R.flatten().astype(\"float32\"))\n",
    "    rndm = gpuarray.to_gpu(np.random.randint(0,len(R),n_iter*n_blocks*n_threads_per_block, dtype=np.int32)) # an array of random numbers used to pick random samples\n",
    "    # Perform stochastic gradient descent on GPU\n",
    "    pycuda_mf_kernel(P, Q, np.float32(alpha), np.float32(beta), np.int32(k), np.int32(len(R)),\n",
    "                       R_gpu, np.int32(n_iter), np.int32(n_threads_per_block), np.int32(n_blocks), rndm,\n",
    "                       grid=(n_blocks,1), block=(n_threads_per_block,1,1))            \n",
    "    return(P,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2bc4a0b4-0345-4b2f-8bcc-f2f19033e177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution: 0.968 s\n",
      "RMSE :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2910410943173563"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "P,Q = mf_sgd_pycuda(R, k=128, alpha=0.021, beta=0.05, n_iter=5, n_threads_per_block=1000, n_blocks=1000)\n",
    "end = time()\n",
    "print(f\"Temps d'exécution: {(end-start):.3f} s\\nRMSE :\")\n",
    "rmse(R, P.get(), Q.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ba5bbe3e-84ef-4a76-a256-293e15e07417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "963 ms ± 2.98 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "P,Q = mf_sgd_pycuda(R, k=128, alpha=0.021, beta=0.05, n_iter=5, n_threads_per_block=1000, n_blocks=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ebc6c-4e9c-4e1d-ad39-08ef6dae701e",
   "metadata": {},
   "source": [
    "De même que les exemples précédents, on a fait ici 5 000 000 updates de gradient (5 itérations $\\times$ 1000 blocks $\\times$ 1000 threads par block) mais cette fois-ci distribués sur 1 000 000 threads, ce qui est largement suffisant pour exploiter la capacité maximale de parallélisation d'un GPU moderne (dans notre cas, on utilise une RTX 3060 dont la capacité de threads concurrents est 28 SM * 32 warps/SM * 32 threads/warp = 28672, mais cette limite est théorique puisqu'une partie des threads peut être indisponible ou occupée par un autre processus).\n",
    "\n",
    "Le temps d'exécution moyen vaut 963ms, on a donc atteint 5 192 107 updates/s, soit un speed up de 5420 % (54.2x) par rapport au code Python non parallélisé.\n",
    "\n",
    "On peut également atteindre des valeurs plus faibles du RMSE en augmentant le nombre d'itérations (tout en conservant un temps d'exécution faible) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4aa57a5d-755c-44bb-ad51-723267b93724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution: 14.080 s\n",
      "RMSE :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.005696545176249839"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "P,Q = mf_sgd_pycuda(R, k=128, alpha=0.012, beta=0.002, n_iter=100, n_threads_per_block=1000, n_blocks=1000)\n",
    "end = time()\n",
    "print(f\"Temps d'exécution: {(end-start):.3f} s\\nRMSE :\")\n",
    "rmse(R, P.get(), Q.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ab6705-ac75-449a-9ce4-e630188f4582",
   "metadata": {},
   "source": [
    "Ici l'overhead d'initialisation devient négligeable et le speed-up par rapport au code Python devient de 74x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef476d9a-3b69-43ef-9b30-6cb6ef7ce3be",
   "metadata": {},
   "source": [
    "Ces résultats sont satisfaisants, mais on peut les améliorer en appliquant l'approche Batch Hogwild! proposée dans l'article [1]. L'idée est de choisir la dimension latente k multiple de 32 (ici 128) et une taille de bloc multiple de 32 threads (chaque groupe de 32 threads est appelé warp) puis à chaque itération, au lieu de traiter une observation aléatoire de R par thread, on traite séquentiellement 128 observations consécutives par warp, et sur chaque warp on parallélise le calcul du gradient et de la nouvelle valeur de P[i] et Q[j] sur les 32 threads du warp. Chaque warp traite donc en parallèle une première observation, puis une deuxième, .., puis une 128ème. Ensuite ce processus est répété n_iter fois.\n",
    "\n",
    "D'après l'article, l'avantage principal de cette approche est l'utilisation plus efficace du cache, avec la mise en cache des 128 triplets consécutifs (i,j,R[i,j]) pour qu'ils soient traités plus rapidement par les 32 threads du warp. D'après nos tests, un autre avantage du traitement des données par batch est qu'on simule une moyennisation du gradient sur le batch ce qui réduit le risque de divergence du gradient. Concrètement, sur les approches précédentes il faut choisir un learning rate alpha de l'ordre de 0.01 sinon le gradient diverge et un overflow se produit sur les valeurs de P et Q, mais avec Batch Hogwild! on peut choisir un learning rate de l'ordre de 0.1 sans observer d'overflow, ce qui accélère la descente de gradient et permet d'obtenir un faible taux d'erreur avec moins d'updates.\n",
    "\n",
    "L'article propose également plusieurs optimisations pour accélérer l'algorithme comme l'utilisation du type half (nombre décimal à 16 bits), et l'instruction \\_shfl\\_down\\_sync  pour transmettre les valeurs P[i,u]Q[j,u] du registre d'un thread à l'autre afin de calculer le produit scalaire P[i].Q[j] sans recourir à la mémoire partagée du GPU (plus lente).\n",
    "\n",
    "Ci-dessous nous proposons une adaptation PyCuda du code Cuda-C [3] fourni par les auteurs de l'article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6733822-4c9c-4de8-82db-e90eadbbb050",
   "metadata": {},
   "outputs": [],
   "source": [
    "pycudakernel = SourceModule(\"\"\"\n",
    "#include <cuda_fp16.h>\n",
    "extern \"C\"{\n",
    "__global__ void pycuda_mf_kernel(half *P, half *Q, float alpha, float beta, int k, int n_samples, int *R, int n_iter, int *rndm)\n",
    "{\n",
    "    int thread_id_within_warp = threadIdx.x%32;\n",
    "    int warp_id = (blockDim.x/32)*blockIdx.x + threadIdx.x/32; // unique combination of block id and warp id within block\n",
    "    int start_id, warp_sample;\n",
    "    for (int iter = 0; iter < n_iter; iter++) {\n",
    "        // set start id (first sample to process) for each warp\n",
    "        // first thread of the warp chooses a random sample and transmits it to the others\n",
    "        if(thread_id_within_warp == 0)\n",
    "        {\n",
    "            start_id = rndm[warp_id*n_iter+iter]; // random int from uniform[O,n_samples] for every warp every iteration\n",
    "        }\n",
    "        start_id = __shfl_sync(0xffffffff, start_id, 0); // fast thread-to-thread transmission within warp\n",
    "        for (warp_sample = 0; warp_sample < k; warp_sample++) {\n",
    "            int offset = (start_id + warp_sample)%n_samples;\n",
    "            int i = __ldg(&R[3*offset]);\n",
    "            int j = __ldg(&R[3*offset+1]);\n",
    "            float r = __ldg(&R[3*offset+2]);\n",
    "            \n",
    "            // read P[i] and Q[j] into register\n",
    "            // there are 128 entries to read, each thread reads 4\n",
    "            int base_p = i*k;\n",
    "            int base_q = j*k;\n",
    "            float tmp_p1 = __half2float(P[base_p + thread_id_within_warp]);\n",
    "            float tmp_q1 = __half2float(Q[base_q + thread_id_within_warp]);\n",
    "\n",
    "            float tmp_p2 = __half2float(P[base_p + thread_id_within_warp + 32]);\n",
    "            float tmp_q2 = __half2float(Q[base_q + thread_id_within_warp + 32]);\n",
    "\n",
    "            float tmp_p3 = __half2float(P[base_p + thread_id_within_warp + 64]);\n",
    "            float tmp_q3 = __half2float(Q[base_q + thread_id_within_warp + 64]);\n",
    "\n",
    "            float tmp_p4 = __half2float(P[base_p + thread_id_within_warp + 96]);\n",
    "            float tmp_q4 = __half2float(Q[base_q + thread_id_within_warp + 96]);\n",
    "            \n",
    "            // calculate dot product on these 4 entries\n",
    "            float tmp_product = tmp_p1*tmp_q1 + tmp_p2*tmp_q2 + tmp_p3*tmp_q3 + tmp_p4*tmp_q4;\n",
    "            \n",
    "            // transmit to other threads and get their products\n",
    "            tmp_product += __shfl_down_sync(0xffffffff, tmp_product, 16);\n",
    "            tmp_product += __shfl_down_sync(0xffffffff, tmp_product, 8);\n",
    "            tmp_product += __shfl_down_sync(0xffffffff, tmp_product, 4);\n",
    "            tmp_product += __shfl_down_sync(0xffffffff, tmp_product, 2);\n",
    "            tmp_product += __shfl_down_sync(0xffffffff, tmp_product, 1);\n",
    "            tmp_product = __shfl_sync(0xffffffff, tmp_product, 0);\n",
    "\n",
    "            float err = r - tmp_product;\n",
    "            \n",
    "            // we need to update k=128 factors in P[i] and Q[j]: each of the 32 threads of the warp updates 4 factors\n",
    "            P[base_p + thread_id_within_warp + 0] = __float2half(tmp_p1 + alpha*(err*tmp_q1 - beta*tmp_p1));\n",
    "            Q[base_q + thread_id_within_warp + 0] = __float2half(tmp_q1 + alpha*(err*tmp_p1 - beta*tmp_q1));\n",
    "\n",
    "            P[base_p + thread_id_within_warp + 32] = __float2half(tmp_p2 + alpha*(err*tmp_q2 - beta*tmp_p2));\n",
    "            Q[base_q + thread_id_within_warp + 32] = __float2half(tmp_q2 + alpha*(err*tmp_p2 - beta*tmp_q2));\n",
    "\n",
    "            P[base_p + thread_id_within_warp + 64] = __float2half(tmp_p3 + alpha*(err*tmp_q3 - beta*tmp_p3));\n",
    "            Q[base_q + thread_id_within_warp + 64] = __float2half(tmp_q3 + alpha*(err*tmp_p3 - beta*tmp_q3));\n",
    "\n",
    "            P[base_p + thread_id_within_warp + 96] = __float2half(tmp_p4 + alpha*(err*tmp_q4 - beta*tmp_p4));\n",
    "            Q[base_q + thread_id_within_warp + 96] = __float2half(tmp_q4 + alpha*(err*tmp_p4 - beta*tmp_q4));\n",
    "        }\n",
    "    }\n",
    "}}\n",
    "  \"\"\", no_extern_c=True)\n",
    "pycuda_mf_kernel = pycudakernel.get_function(\"pycuda_mf_kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80543cea-fd77-427c-b082-86d12c3dab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mf_sgd_pycuda(R, k, alpha, beta, n_iter, n_threads_per_block, n_blocks ):\n",
    "    \"\"\"\n",
    "    Matrix factorization\n",
    "    - R : matrix to factorize\n",
    "    - k : latent dimension\n",
    "    - alpha : learning rate\n",
    "    - beta : regularization parameter\n",
    "    - n_iter : number of SGD iterations\n",
    "    - n_threads_per_block : number of threads per block, each thread will run the kernel code\n",
    "    - n_blocks : number of blocks (groups of threads)\n",
    "    \"\"\"\n",
    "    # Copy the needed arrays to GPU memory\n",
    "    m = 1 + np.max(R[:,0])\n",
    "    n = 1 + np.max(R[:,1])\n",
    "    P = gpuarray.to_gpu((1./k * np.random.normal(size = (m,k))).astype(\"float16\"))\n",
    "    Q = gpuarray.to_gpu((1./k * np.random.normal(size = (n,k))).astype(\"float16\"))\n",
    "    R_gpu = gpuarray.to_gpu(R.flatten().astype(\"int32\"))\n",
    "    rndm = gpuarray.to_gpu(np.random.randint(0,len(R),n_iter*n_blocks*n_threads_per_block//32, dtype=np.int32))\n",
    "    # Perform stochastic gradient descent on GPU\n",
    "    pycuda_mf_kernel(P, Q, np.float32(alpha), np.float32(beta), np.int32(k), np.int32(len(R)),\n",
    "                       R_gpu, np.int32(n_iter), rndm,\n",
    "                       grid=(n_blocks,1), block=(n_threads_per_block,1,1))\n",
    "    return(P,Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "3c1b317e-1969-4be0-a819-0acfd21c6cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution: 0.274 s\n",
      "RMSE :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.001062024815507108"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "P,Q = mf_sgd_pycuda(R, k=128, alpha=0.13, beta=0.0001, n_iter=2, n_threads_per_block=1024, n_blocks=610)\n",
    "end = time()\n",
    "print(f\"Temps d'exécution: {(end-start):.3f} s\\nRMSE :\")\n",
    "rmse(R, P.get().astype(\"float32\"), Q.get().astype(\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "190a2e12-1bf4-4234-9147-c3633aa0bdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270 ms ± 1.28 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "P,Q = mf_sgd_pycuda(R, k=128, alpha=0.13, beta=0.0001, n_iter=2, n_threads_per_block=1024, n_blocks=610)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e58765-adc5-42ac-b477-fb9bc453f643",
   "metadata": {},
   "source": [
    "Dans l'exemple précédent on a choisi n_threads_per_block = 1024, n_blocks = 610 et n_iter = 2 pour avoir n_updates =  n_iter $\\times$ n_blocks $\\times$ 128 $\\times$ n_threads_per_block/32 = 4 997 120 updates, en ligne avec les méthodes précédentes. On obtient 18,3 millions updates/s, soit 3.5x de plus que Hogwild! standard, mais on peut atteindre un meilleur speed-up en augmentant le nombre d'itérations, car ici le code s'exécute si rapidement (270 ms) que l'overhead pour initialiser P et Q et transmettre les données en mémoire GPU n'est plus négligeable. On augmente donc le nombre d'itérations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8d4f1131-8b2e-466e-b7ea-b3b933d0633b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution: 8.270 s\n",
      "RMSE :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0003738241412920352"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "P,Q = mf_sgd_pycuda(R, k=128, alpha=0.12, beta=0.0001, n_iter=1000, n_threads_per_block=1024, n_blocks=1024)\n",
    "end = time()\n",
    "print(f\"Temps d'exécution: {(end-start):.3f} s\\nRMSE :\")\n",
    "rmse(R, P.get().astype(\"float32\"), Q.get().astype(\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2e054aae-9630-4af2-af19-aca8144e6be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.12 s ± 5.13 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "P,Q = mf_sgd_pycuda(R, k=128, alpha=0.12, beta=0.0001, n_iter=1000, n_threads_per_block=1024, n_blocks=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f24b5f-a9bf-49d4-90b1-f673de41b5d9",
   "metadata": {},
   "source": [
    "Ici on effectue 1000 $\\times$ 1024 $\\times$ 128 $\\times$ 1024 / 32 = 4 194 304 000 updates, soit 516 539 901 updates/s ce qui est très satisfaisant et représente un speed-up de 5392x par rapport à l'implémentation séquentielle Python (sparse).\n",
    "\n",
    "On s'approche de la limite de notre GPU qui est de 12.7 TFLOPS (qui serait équivalent à 516M updates/s si on suppose que chaque update utilise l'équivalent de 24586 opérations en virgule flottante, ce qui est peut-être un seul ordre de grandeur supérieur aux opérations qu'on effectue ici pour k = 128)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc484713-2928-4247-802c-911dfa9d79eb",
   "metadata": {},
   "source": [
    "## CuPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86e4702-a1c0-4846-8bab-36e0fd4ba606",
   "metadata": {},
   "source": [
    "CuPy est un module Python qui propose des objets (principalement arrays) similaires à Numpy mais dont les méthodes sont parallélisées sur GPU avec CUDA. Dans notre cas, remplacer Numpy par CuPy dans notre code Python séquentiel n'a pas amélioré son temps d'exécution (car les boucles for de Python sont lentes et non parallélisées), mais CuPy offre également la possibilité d'écrire des kernels CUDA tout comme PyCuda. Pour comparer PyCuda et CuPy, on teste donc notre code précédent (batch Hogwild!) sur CuPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4a0a6761-ac70-4e8e-b6f4-7185eff8bfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cupy_mf_kernel = cp.RawKernel(\"\"\"\n",
    "#include <cuda_fp16.h>\n",
    "extern \"C\"\n",
    "__global__ void cupy_mf_kernel(half *P, half *Q, float alpha, float beta, int k, int n_samples, int *R, int n_iter, int *rndm)\n",
    "{\n",
    "    int thread_id_within_warp = threadIdx.x%32;\n",
    "    int warp_id = (blockDim.x/32)*blockIdx.x + threadIdx.x/32; // unique combination of block id and warp id within block\n",
    "    int start_id, warp_sample;\n",
    "    for (int iter = 0; iter < n_iter; iter++) {\n",
    "        // set start id (first sample to process) for each warp\n",
    "        // first thread of the warp chooses a random sample and transmits it to the others\n",
    "        if(thread_id_within_warp == 0)\n",
    "        {\n",
    "            start_id = rndm[warp_id*n_iter+iter]; // random int from uniform[O,n_samples] for every warp every iteration\n",
    "        }\n",
    "        start_id = __shfl_sync(0xffffffff, start_id, 0); // fast thread-to-thread transmission within warp\n",
    "        for (warp_sample = 0; warp_sample < k; warp_sample++) {\n",
    "            int offset = (start_id + warp_sample)%n_samples;\n",
    "            int i = __ldg(&R[3*offset]);\n",
    "            int j = __ldg(&R[3*offset+1]);\n",
    "            float r = __ldg(&R[3*offset+2]);\n",
    "            \n",
    "            // read P[i] and Q[j] into register\n",
    "            // there are 128 entries to read, each thread reads 4\n",
    "            int base_p = i*k;\n",
    "            int base_q = j*k;\n",
    "            float tmp_p1 = __half2float(P[base_p + thread_id_within_warp]);\n",
    "            float tmp_q1 = __half2float(Q[base_q + thread_id_within_warp]);\n",
    "\n",
    "            float tmp_p2 = __half2float(P[base_p + thread_id_within_warp + 32]);\n",
    "            float tmp_q2 = __half2float(Q[base_q + thread_id_within_warp + 32]);\n",
    "\n",
    "            float tmp_p3 = __half2float(P[base_p + thread_id_within_warp + 64]);\n",
    "            float tmp_q3 = __half2float(Q[base_q + thread_id_within_warp + 64]);\n",
    "\n",
    "            float tmp_p4 = __half2float(P[base_p + thread_id_within_warp + 96]);\n",
    "            float tmp_q4 = __half2float(Q[base_q + thread_id_within_warp + 96]);\n",
    "            \n",
    "            // calculate dot product on these 4 entries\n",
    "            float tmp_product = tmp_p1*tmp_q1 + tmp_p2*tmp_q2 + tmp_p3*tmp_q3 + tmp_p4*tmp_q4;\n",
    "            \n",
    "            // transmit to other threads and get their products\n",
    "            tmp_product += __shfl_down_sync(0xffffffff, tmp_product, 16);\n",
    "            tmp_product += __shfl_down_sync(0xffffffff, tmp_product, 8);\n",
    "            tmp_product += __shfl_down_sync(0xffffffff, tmp_product, 4);\n",
    "            tmp_product += __shfl_down_sync(0xffffffff, tmp_product, 2);\n",
    "            tmp_product += __shfl_down_sync(0xffffffff, tmp_product, 1);\n",
    "            tmp_product = __shfl_sync(0xffffffff, tmp_product, 0);\n",
    "\n",
    "            float err = r - tmp_product;\n",
    "            \n",
    "            // we need to update k=128 factors in P[i] and Q[j]: each of the 32 threads of the warp updates 4 factors\n",
    "            P[base_p + thread_id_within_warp + 0] = __float2half(tmp_p1 + alpha*(err*tmp_q1 - beta*tmp_p1));\n",
    "            Q[base_q + thread_id_within_warp + 0] = __float2half(tmp_q1 + alpha*(err*tmp_p1 - beta*tmp_q1));\n",
    "\n",
    "            P[base_p + thread_id_within_warp + 32] = __float2half(tmp_p2 + alpha*(err*tmp_q2 - beta*tmp_p2));\n",
    "            Q[base_q + thread_id_within_warp + 32] = __float2half(tmp_q2 + alpha*(err*tmp_p2 - beta*tmp_q2));\n",
    "\n",
    "            P[base_p + thread_id_within_warp + 64] = __float2half(tmp_p3 + alpha*(err*tmp_q3 - beta*tmp_p3));\n",
    "            Q[base_q + thread_id_within_warp + 64] = __float2half(tmp_q3 + alpha*(err*tmp_p3 - beta*tmp_q3));\n",
    "\n",
    "            P[base_p + thread_id_within_warp + 96] = __float2half(tmp_p4 + alpha*(err*tmp_q4 - beta*tmp_p4));\n",
    "            Q[base_q + thread_id_within_warp + 96] = __float2half(tmp_q4 + alpha*(err*tmp_p4 - beta*tmp_q4));\n",
    "        }\n",
    "    }\n",
    "}\n",
    "  \"\"\", \"cupy_mf_kernel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "08173acb-0c5d-4a98-a290-46ad0b173527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mf_sgd_cupy(R, k, alpha, beta, n_iter, n_threads_per_block, n_blocks):\n",
    "    \"\"\"\n",
    "    Matrix factorization\n",
    "    - R : matrix to factorize\n",
    "    - k : latent dimension\n",
    "    - alpha : learning rate\n",
    "    - beta : regularization parameter\n",
    "    - n_iter : number of SGD iterations\n",
    "    - n_threads_per_block : number of threads per block, each thread will run the kernel code\n",
    "    - n_blocks : number of blocks (groups of threads)\n",
    "    \"\"\"\n",
    "    # Initialize user and item latent feature matrice\n",
    "    m = 1 + np.max(R[:,0])\n",
    "    n = 1 + np.max(R[:,1])\n",
    "    P = 1./k * cp.random.rand(m, k).astype(cp.half)\n",
    "    Q = 1./k * cp.random.rand(n, k).astype(cp.half)\n",
    "    R_gpu = cp.asarray(R.flatten(), dtype=cp.int32)\n",
    "    rndm = cp.random.randint(0,len(R),n_iter*n_blocks*n_threads_per_block//32, dtype=cp.int32)\n",
    "    # Perform stochastic gradient descent on GPU\n",
    "    cupy_mf_kernel((n_blocks,), (n_threads_per_block,), (P, Q, np.float32(alpha), np.float32(beta), np.int32(k), np.int32(len(R)), R_gpu, np.int32(n_iter), rndm))\n",
    "    return(P,Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23db50d-2ed0-41d4-bd73-aecac8cd85c6",
   "metadata": {},
   "source": [
    "Premier test avec 4 997 120 updates de gradient :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0cb08e33-86bd-4b47-8b14-cf32cae066c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution: 0.014 s\n",
      "RMSE :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0026886741770251704"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "P,Q = mf_sgd_cupy(R, k=128, alpha=0.13, beta=0.0001, n_iter=2, n_threads_per_block=1024, n_blocks=610)\n",
    "print(P[0,0]) # CuPy doesn't run the kernel code until you print the result\n",
    "end = time()\n",
    "clear_output()\n",
    "print(f\"Temps d'exécution: {(end-start):.3f} s\\nRMSE :\")\n",
    "rmse(R, P.get().astype(\"float32\"), Q.get().astype(\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "58b2c042-ea11-418a-bc8a-6eb784a0f967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.2 ms ± 41.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "P,Q = mf_sgd_cupy(R, k=128, alpha=0.13, beta=0.0001, n_iter=2, n_threads_per_block=1024, n_blocks=610)\n",
    "print(P[0,0])\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b23af7f-2140-4af1-988e-ecd594a7680e",
   "metadata": {},
   "source": [
    "Deuxième test avec 4 194 304 000 updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "dadb6fa8-784a-44dc-8886-cc897ac093fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution: 7.810 s\n",
      "RMSE :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0005486438305574281"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "P,Q = mf_sgd_cupy(R, k=128, alpha=0.12, beta=0.0001, n_iter=1000, n_threads_per_block=1024, n_blocks=1024)\n",
    "print(P[0,0]) # CuPy doesn't run the kernel code until you print the result\n",
    "end = time()\n",
    "clear_output()\n",
    "print(f\"Temps d'exécution: {(end-start):.3f} s\\nRMSE :\")\n",
    "rmse(R, P.get().astype(\"float32\"), Q.get().astype(\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a8a0b636-60ae-49a8-9b56-6a504c0d1803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.64 s ± 36.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "P,Q = mf_sgd_cupy(R, k=128, alpha=0.12, beta=0.0001, n_iter=1000, n_threads_per_block=1024, n_blocks=1024)\n",
    "print(P[0,0])\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a1dd3-f2b2-496d-bb17-4b5817e688e2",
   "metadata": {},
   "source": [
    "On constate que CuPy garde un nombre d'updates/s élevé dans les deux tests (respectivement 410 M et 549 M) ce qui signifie qu'il encourt un overhead d'exécution significativement plus faible que PyCuda. Sur le deuxième test on obtient un speed-up de 6% par rapport à PyCuda et de 5732x par rapport à l'implémentation Python séquentielle.\n",
    "\n",
    "Curieusement, les paramètres alpha et beta optimaux pour PyCuda ne correspondent pas exactement aux paramètres optimaux pour CuPy bien que le code du kernel soit identique, et on obtient une erreur légèrement plus élevée avec CuPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d38eba13-59b5-4e77-8ff8-fb0f92affc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00041243791095700767"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P,Q = mf_sgd_cupy(R, k=128, alpha=0.16, beta=0.00001, n_iter=1000, n_threads_per_block=1024, n_blocks=1024)\n",
    "print(P[0,0])\n",
    "clear_output()\n",
    "print(\"RMSE :\")\n",
    "rmse(R, P.get().astype(\"float32\"), Q.get().astype(\"float32\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d51879b-a018-4675-8f31-a8968e4039bb",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Implémentation sparse séquentielle Python (référence): 95785 updates/s.\n",
    "\n",
    "Numba : Speed-up de 1.62 (62% plus rapide)\n",
    "\n",
    "PyCuda Hogwild! standard: Speed-up de 74\n",
    "\n",
    "PyCuda Batch Hogwild! optimisé : Speed-up de 5392\n",
    "\n",
    "CuPy Batch Hogwild! optimisé : Speed-up de 5732\n",
    "\n",
    "Remarque: résultats susceptibles de varier en fonction de la machine utilisée"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9468aff-fdb7-4d71-8e99-2ede82a196ea",
   "metadata": {},
   "source": [
    "### Références\n",
    "[1] Xiaolong Xie, Wei Tan, Liana Fong, Yun Liang: CuMF_SGD: Parallelized Stochastic Gradient Descent for Matrix Factorization on GPUs: https://arxiv.org/abs/1610.05838"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c04be-54e1-4cc9-8e77-5d8974d3c438",
   "metadata": {},
   "source": [
    "[2] Feng Niu, Benjamin Recht, Christopher Re, Stephen J. Wright: HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent: https://arxiv.org/abs/1106.5730"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a0f12b-5bfd-4da8-bb8f-af3f3b617519",
   "metadata": {},
   "source": [
    "[3] GitHub CuMF SGD https://github.com/cuMF/cumf_sgd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
